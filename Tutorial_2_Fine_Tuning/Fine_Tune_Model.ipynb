{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad17e74",
   "metadata": {},
   "source": [
    "# üî¨ Tutorial 2: Model Fine-Tuning\n",
    "## Adapting ESM-2 for Protein Fitness Prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Load and use pre-trained protein language models (ESM-2)\n",
    "- Generate protein embeddings for downstream tasks\n",
    "- Perform zero-shot similarity search\n",
    "- Fine-tune models with LoRA (Low-Rank Adaptation)\n",
    "- Predict DMS stability scores using adapted models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5d04e",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Setup and Imports\n",
    "\n",
    "We'll use:\n",
    "- **`transformers`**: Load pre-trained ESM models from Hugging Face\n",
    "- **`peft`**: Apply LoRA adapters for parameter-efficient fine-tuning\n",
    "- **`torch`**: Deep learning framework for training\n",
    "- **`scikit-learn`**: Metrics and baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2d827-14a2-452b-9c37-e982a2ef3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Utilities\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style for prettier plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "\n",
    "if cwd.name == \"Tutorial_2_Fine_Tuning\":\n",
    "    BASE_DIR = Path(\"..\")\n",
    "elif cwd.name == \"tutorials\":\n",
    "    BASE_DIR = Path(\"./\")\n",
    "elif cwd == Path(\"/workspace\"):\n",
    "    BASE_DIR = Path(\"/workspace/tutorials\")\n",
    "else:\n",
    "    BASE_DIR = cwd\n",
    "\n",
    "print(f\"Current directory:     {cwd}\")\n",
    "print(f\"Base directory set to: {BASE_DIR.resolve()}\")\n",
    "\n",
    "dataset_path = BASE_DIR / Path(\"ProteinGym_DMS_data\") / \"cleaned_ProteinGym_DMS_substitutions.csv\"\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"Warning:               Running from notebook directory but data file not found\")\n",
    "    print(f\"Expected:              {dataset_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data file is not found, download and extract it\n",
    "if not dataset_path.exists():\n",
    "    import tarfile\n",
    "    import urllib.request\n",
    "    \n",
    "    print(f\"üì• Data file not found. Downloading...\")\n",
    "    \n",
    "    # URL and local paths\n",
    "    download_url = \"https://raw.githubusercontent.com/ai-for-science-org/tutorials/refs/heads/main/tutorial_data/cleaned_ProteinGym_DMS_substitutions.csv.tar.gz\"\n",
    "    tar_gz_path = BASE_DIR / Path(\"ProteinGym_DMS_data\") / \"cleaned_ProteinGym_DMS_substitutions.csv.tar.gz\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    tar_gz_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download the file\n",
    "    print(f\"  Downloading from: {download_url}\")\n",
    "    urllib.request.urlretrieve(download_url, tar_gz_path)\n",
    "    print(f\"  ‚úì Downloaded to: {tar_gz_path}\")\n",
    "    \n",
    "    # Extract the tar.gz file\n",
    "    print(f\"  Extracting archive...\")\n",
    "    with tarfile.open(tar_gz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=tar_gz_path.parent)\n",
    "    print(f\"  ‚úì Extracted successfully\")\n",
    "    \n",
    "    # Clean up the tar.gz file\n",
    "    tar_gz_path.unlink()\n",
    "    print(f\"  ‚úì Removed archive file\")\n",
    "    \n",
    "    print(f\"\\n‚úì Data file available at: {dataset_path}\")\n",
    "else:\n",
    "    print(f\"‚úì Data file found at: {dataset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ea7c4",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Load Pre-trained ESM Model\n",
    "\n",
    "We'll use the ESM-2 model from Meta AI/Facebook Research. ESM-2 models are protein language models trained on millions of sequences to understand protein structure and function.\n",
    "\n",
    "### Available ESM-2 Models\n",
    "\n",
    "| Model Checkpoint | Layers | Parameters | Best For |\n",
    "|:-----------------|-------:|-----------:|:---------|\n",
    "| `esm2_t48_15B_UR50D` | 48 | 15B | Maximum accuracy (requires significant GPU) |\n",
    "| `esm2_t36_3B_UR50D` | 36 | 3B | High accuracy, large scale |\n",
    "| `esm2_t33_650M_UR50D` | 33 | 650M | Good balance of accuracy and speed |\n",
    "| `esm2_t30_150M_UR50D` | 30 | 150M | Medium-sized tasks |\n",
    "| `esm2_t12_35M_UR50D` | 12 | 35M | Fast experimentation |\n",
    "| `esm2_t6_8M_UR50D` | 6 | 8M | **Tutorial choice** - fastest training |\n",
    "\n",
    "üí° **Tip**: Start with smaller models for prototyping, then scale up for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model checkpoint (change this to experiment with different sizes)\n",
    "MODEL_CHECKPOINT = \"facebook/esm2_t6_8M_UR50D\"  # Smallest ESM-2 model (8M parameters)\n",
    "\n",
    "print(f\"üì• Loading model: {MODEL_CHECKPOINT}\")\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(device)\n",
    "model.eval()  # Set to evaluation mode (no training yet)\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úì Model loaded successfully!\")\n",
    "print(f\"  Total parameters: {total_params/1e6:.1f} million\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Number of layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c318bd",
   "metadata": {},
   "source": [
    "## üî§ Step 3: Understanding the Tokenizer\n",
    "\n",
    "Protein sequences are represented as strings of single-letter amino acid codes. The **tokenizer** converts these strings into numerical representations that models can process.\n",
    "\n",
    "**What it does:**\n",
    "- Converts amino acid letters to numerical token IDs\n",
    "- Adds special tokens (start, end, padding)\n",
    "- Handles sequence padding and truncation\n",
    "- Ensures all sequences in a batch have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "print(\"‚úì Tokenizer loaded!\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "print(\"\\nüìñ Tokenizer vocabulary:\")\n",
    "print(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9c37c",
   "metadata": {},
   "source": [
    "Let's create a visualization of what the tokenizer does.\n",
    "\n",
    "First, collect the tokenizer vocabulary using the `vocab = tokenizer.get_vocab()` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523305a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tokenizer vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Separate special tokens from amino acid tokens\n",
    "special_tokens = {k: v for k, v in vocab.items() if '<' in k or '|' in k}\n",
    "amino_acid_tokens = {k: v for k, v in vocab.items() if k not in special_tokens and len(k) == 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff477f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Amino Acid Token IDs\n",
    "aa_names = list(amino_acid_tokens.keys())\n",
    "aa_ids = list(amino_acid_tokens.values())\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(aa_names)))\n",
    "\n",
    "ax1.barh(aa_names, aa_ids, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Token ID', fontsize=11)\n",
    "ax1.set_ylabel('Amino Acid', fontsize=11)\n",
    "ax1.set_title('Amino Acid Token IDs', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Plot 2: Special Tokens\n",
    "special_names = list(special_tokens.keys())\n",
    "special_ids = list(special_tokens.values())\n",
    "special_colors = ['coral', 'steelblue', 'seagreen', 'gold', 'mediumpurple']\n",
    "\n",
    "bars = ax2.barh(special_names, special_ids, color=special_colors[:len(special_names)], \n",
    "                edgecolor='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Token ID', fontsize=11)\n",
    "ax2.set_ylabel('Special Token', fontsize=11)\n",
    "ax2.set_title('Special Tokens', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Add value labels on special tokens\n",
    "for i, (name, val) in enumerate(zip(special_names, special_ids)):\n",
    "    ax2.text(val + 0.5, i, str(val), va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìñ Token Types:\")\n",
    "print(f\"  Amino acids: {len(amino_acid_tokens)} tokens (standard 20 + variants)\")\n",
    "print(f\"  Special tokens: {len(special_tokens)} tokens\")\n",
    "print(f\"  Total vocabulary: {len(vocab)} tokens\")\n",
    "\n",
    "print(\"\\nüí° Special Tokens:\")\n",
    "for name, token_id in special_tokens.items():\n",
    "    descriptions = {\n",
    "        '<pad>': 'Padding token (fills sequences to same length)',\n",
    "        '<eos>': 'End of sequence marker',\n",
    "        '<unk>': 'Unknown token (for invalid amino acids)',\n",
    "        '<cls>': 'Start of sequence marker',\n",
    "        '<mask>': 'Mask token (for training)',\n",
    "    }\n",
    "    desc = descriptions.get(name, 'Special token')\n",
    "    print(f\"  {name:8s} (ID {token_id:2d}): {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cede86",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization Example\n",
    "\n",
    "Let's tokenize a sample protein sequence to see the tokenizer in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d24b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample protein sequence: Human Insulin\n",
    "sequence = \"MALWMRLLPLLALLALWGPDPAAAFVNQHLCGSHLVEALYLVCGERGFFYTPKTRREAEDLQVGQVELGGGPGAGSLQPLALEGSLQKRGIVEQCCTSICSLYQLENYCN\"\n",
    "\n",
    "print(\"Sample sequence (Human Insulin):\")\n",
    "print(f\"  Length: {len(sequence)} amino acids\")\n",
    "print(f\"  Sequence: {sequence[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3965a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sequence\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"‚úì Sequence tokenized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efcd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine tokenized output\n",
    "print(f\"Original sequence length: {len(sequence)} amino acids\")\n",
    "print(f\"Tokenized input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Token IDs: {inputs['input_ids'][0][:20]}...\")  # Show first 20 tokens\n",
    "\n",
    "print(f\"\\nüí° Why is tokenized length different?\")\n",
    "print(f\"   The tokenizer adds special tokens like <cls> (start) and <eos> (end)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078bfba",
   "metadata": {},
   "source": [
    "## üß¨ Step 4: Generating Protein Embeddings\n",
    "\n",
    "ESM-2 models generate rich vector representations (**embeddings**) that capture structural and functional properties of proteins.\n",
    "\n",
    "**Two types of embeddings:**\n",
    "1. **Per-token embeddings**: A vector for each amino acid in the sequence\n",
    "2. **Pooled embedding**: A single vector representing the entire protein\n",
    "\n",
    "These embeddings can be used for:\n",
    "- Similarity search\n",
    "- Function prediction\n",
    "- Structure prediction\n",
    "- Mutation effect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb758eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for our sample sequence\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract both types of embeddings\n",
    "per_token_embeddings = outputs.last_hidden_state\n",
    "pooled_embedding = outputs.pooler_output\n",
    "\n",
    "print(\"‚úì Embeddings generated!\\n\")\n",
    "print(\"üìä Per-token Embeddings\")\n",
    "print(f\"   Shape: {per_token_embeddings.shape}\")\n",
    "print(f\"   ‚Üí (batch_size, sequence_length, hidden_dim)\")\n",
    "print(f\"   ‚Üí One vector for each amino acid in the sequence\\n\")\n",
    "\n",
    "print(\"üìä Pooled (Whole-Sequence) Embedding\")\n",
    "print(f\"   Shape: {pooled_embedding.shape}\")\n",
    "print(f\"   ‚Üí (batch_size, hidden_dim)\")\n",
    "print(f\"   ‚Üí Single vector representing the entire protein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the pooled embedding\n",
    "print(\"Pooled embedding values (first 20 dimensions):\")\n",
    "print(pooled_embedding[0, :20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed6693",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Zero-Shot Prediction\n",
    "\n",
    "The model was trained on masked language modeling (predicting missing amino acids), but the embeddings can be used **directly** for downstream tasks without additional training. This is called **zero-shot learning**.\n",
    "\n",
    "**Use case**: Find proteins similar in function or structure based on embedding similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a401907",
   "metadata": {},
   "source": [
    "### 5.1 Create Reference Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference database of known proteins with different functions\n",
    "reference = {\n",
    "    \"Human Lysozyme (Enzyme)\": \n",
    "        \"MKALIVLGLVLLSVTVQGKVFERCELARTLKRLGMDGYRGISLANWMCLAKWESGYNTRATNYNAGDRSTDYGIFQINSRYWCNDGKTPGAVNACHLSCSALLQDNIADAVACAKRVVRDPQGIRAWVAWRNRCQNRDVRQYVQGCGV\",\n",
    "    \n",
    "    \"Human Hemoglobin (Oxygen Transport)\": \n",
    "        \"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",\n",
    "    \n",
    "    \"Human Keratin (Structural Protein)\": \n",
    "        \"MTTCSRQFTSSSSMKGSCGIGGGIGGGSSRISSVLAGGSCRAPSTYGGGLSVSSSRFSSGGACGLGGGYGGGFSSSSSSFGSGFGGGYGGGLGAGLGGGFGGGFAGGDGLLVGSEKVTMQNLNDRLASYLDKVRALEEANADLEVKIRDWYQRQRPAEIKDYSPYFKTIEDLRNKILTATVDNANVLLQIDNARLAADDFRTKYETELNLRMSVEADINGLRRVLDELTLARADLEMQIESLKEELAYLKKNHEEEMNALRGQVGGDVNVEMDAAPGVDLSRILNEMRDQYEKMAEKNRKDAEEWFFTKTEELNREVATNSELVQSGKSEISELRRTMQNLEIELQSQLSMKASLENSLEETKGRYCMQLAQIQEMIGSVEEQLAQLRCEMEQQNQEYKILLDVKTRLEQEIATYRRLLEGEDAHLSSSQFSSGSQSSRDVTSSSRQIRTKVMDVHDGKVVSTHEQVLRTKN\"\n",
    "}\n",
    "\n",
    "print(f\"‚úì Reference database created with {len(reference)} proteins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca584f3",
   "metadata": {},
   "source": [
    "### 5.2 Define Query Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query protein - which reference protein is it most similar to?\n",
    "query = {\n",
    "    \"Human Myoglobin (Oxygen Storage)\": \n",
    "        \"MGLSDGEWQLVLNVWGKVEADIPGHGQEVLIRLFKGHPETLEKFDKFKHLKSEDEMKASEDLKKHGATVLTALGGILKKKGHHEAEIKPLAQSHATKHKIPVKYLEFISECIIQVLQSKHPGDFGADAQGAMNKALELFRKDMASNYKELGFQG\"\n",
    "}\n",
    "\n",
    "print(f\"üîç Query protein: {list(query.keys())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d5af8",
   "metadata": {},
   "source": [
    "### 5.3 Generate Embeddings for Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21554db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for reference database\n",
    "print(\"üîÑ Generating embeddings for reference database...\")\n",
    "reference_embeddings = {}\n",
    "with torch.no_grad():\n",
    "    for name, seq in reference.items():\n",
    "        ref_inputs = tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "        ref_outputs = model(**ref_inputs)\n",
    "        reference_embeddings[name] = ref_outputs.pooler_output.cpu().numpy()\n",
    "print(f\"‚úì Generated {len(reference_embeddings)} reference embeddings\")\n",
    "\n",
    "# Generate embedding for query protein\n",
    "query_name, query_sequence = next(iter(query.items()))\n",
    "print(f\"\\nüîÑ Generating embedding for query: {query_name}...\")\n",
    "with torch.no_grad():\n",
    "    query_inputs = tokenizer(query_sequence, return_tensors=\"pt\").to(device)\n",
    "    query_outputs = model(**query_inputs)\n",
    "    query_embedding = query_outputs.pooler_output.cpu().numpy()\n",
    "print(\"‚úì Query embedding generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635124f",
   "metadata": {},
   "source": [
    "### 5.4 Calculate Similarity Scores\n",
    "\n",
    "We'll use **cosine similarity** to measure how similar the query protein is to each reference protein.\n",
    "\n",
    "**Cosine similarity** measures the cosine of the angle between two vectors:\n",
    "- **1.0**: Identical proteins\n",
    "- **0.0**: Unrelated\n",
    "- **-1.0**: Opposite (rare in practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7ff50-31c7-454e-9471-8aa96ad9f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between query and each reference\n",
    "similarities = {}\n",
    "for name, ref_emb in reference_embeddings.items():\n",
    "    similarity = cosine_similarity(query_embedding, ref_emb)[0][0]\n",
    "    similarities[name] = similarity\n",
    "\n",
    "# Find best match\n",
    "best_match = max(similarities, key=similarities.get)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üîç ZERO-SHOT SIMILARITY SEARCH RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nQuery Protein: {query_name}\\n\")\n",
    "print(\"Similarity Scores (sorted by relevance):\")\n",
    "print(\"-\" * 70)\n",
    "for name, score in sorted(similarities.items(), key=lambda item: item[1], reverse=True):\n",
    "    bar = \"‚ñà\" * int(score * 50)\n",
    "    print(f\"{name:45s} {score:.4f} {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üéØ Best Match: {best_match}\")\n",
    "print(f\"   Similarity: {similarities[best_match]:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° Myoglobin and Hemoglobin are both oxygen-binding proteins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb2e0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Step 6: Fine-Tuning for Specific Tasks\n",
    "\n",
    "Zero-shot works well for similarity, but what about predicting specific properties like **protein stability**?\n",
    "\n",
    "**Goal**: Predict DMS (Deep Mutational Scanning) scores that measure protein fitness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ebfee",
   "metadata": {},
   "source": [
    "### 6.1 Load and Prepare Dataset\n",
    "\n",
    "We'll use the cleaned ProteinGym dataset from Tutorial 1, which contains:\n",
    "- **Mutated sequences**: Protein variants\n",
    "- **DMS scores**: Experimental fitness measurements (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48249bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset from Tutorial 1\n",
    "DATASET_NAME = BASE_DIR / \"ProteinGym_DMS_data\" / \"cleaned_ProteinGym_DMS_substitutions.csv\"\n",
    "\n",
    "print(f\"üìÇ Loading dataset: {DATASET_NAME}\")\n",
    "dataset = pd.read_csv(DATASET_NAME, usecols=[\"mut_seq\", \"score_norm\"])\n",
    "\n",
    "print(f\"‚úì Dataset loaded successfully!\")\n",
    "print(f\"  Total samples: {len(dataset):,}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c69b1",
   "metadata": {},
   "source": [
    "### 6.2 Create Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NSAMPLES = 50000  # Samples per split\n",
    "SEED = 42\n",
    "\n",
    "# Create random splits\n",
    "rng = np.random.default_rng(SEED)\n",
    "random_idxs = rng.choice(len(dataset), len(dataset), replace=False)\n",
    "\n",
    "train_df = dataset.iloc[random_idxs[:NSAMPLES]]\n",
    "val_df = dataset.iloc[random_idxs[NSAMPLES:2*NSAMPLES]]\n",
    "test_df = dataset.iloc[random_idxs[2*NSAMPLES:3*NSAMPLES]]\n",
    "\n",
    "print(\"‚úì Data split created:\")\n",
    "print(f\"  Train set: {len(train_df):,} samples\")\n",
    "print(f\"  Validation set: {len(val_df):,} samples\")\n",
    "print(f\"  Test set: {len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109ffc0",
   "metadata": {},
   "source": [
    "## üìà Step 7: Baseline - Linear Regression on Frozen Embeddings\n",
    "\n",
    "**First approach**: Use embeddings from the frozen (unchanged) model and train a simple linear regression on top.\n",
    "\n",
    "This is fast but limited - the embeddings weren't optimized for our specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40808240",
   "metadata": {},
   "source": [
    "### 7.1 Set Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cf887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all random seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"‚úì Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119fd2c",
   "metadata": {},
   "source": [
    "### 7.2 Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset for protein sequences\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that tokenizes protein sequences on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sequence = row[\"mut_seq\"]\n",
    "        label = torch.tensor(row[\"score_norm\"], dtype=torch.float32)\n",
    "\n",
    "        # Tokenize sequence\n",
    "        inputs = self.tokenizer(\n",
    "            sequence,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        # Remove batch dimension\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, label\n",
    "\n",
    "print(\"‚úì SequenceDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ec910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = SequenceDataset(train_df, tokenizer)\n",
    "val_dataset = SequenceDataset(val_df, tokenizer)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer)\n",
    "\n",
    "print(\"‚úì Datasets created:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples\")\n",
    "print(f\"  Val: {len(val_dataset):,} samples\")\n",
    "print(f\"  Test: {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46b9b3",
   "metadata": {},
   "source": [
    "### 7.3 Generate Embeddings with Frozen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815607fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 12 if device.type != 'mps' else 0  # Avoid multiprocessing issues on macOS MPS\n",
    "\n",
    "print(\"üîÑ Generating embeddings with frozen model...\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "# Generate embeddings for training and test sets\n",
    "train_embeddings = []\n",
    "test_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Training embeddings\n",
    "    print(\"üìä Processing training set...\")\n",
    "    for batch in tqdm(DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)):\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        train_embeddings.append(outputs.pooler_output.cpu().numpy())\n",
    "    train_embeddings = np.vstack(train_embeddings)\n",
    "    print(f\"‚úì Training embeddings: {train_embeddings.shape}\\n\")\n",
    "\n",
    "    # Test embeddings\n",
    "    print(\"üìä Processing test set...\")\n",
    "    for batch in tqdm(DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)):\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        test_embeddings.append(outputs.pooler_output.cpu().numpy())\n",
    "    test_embeddings = np.vstack(test_embeddings)\n",
    "    print(f\"‚úì Test embeddings: {test_embeddings.shape}\\n\")\n",
    "\n",
    "print(\"‚úì All embeddings generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927df5ed",
   "metadata": {},
   "source": [
    "### 7.4 Train Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression on frozen embeddings\n",
    "print(\"üîÑ Training linear regression model...\")\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(train_embeddings, train_df[\"score_norm\"].values)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = linear_model.predict(test_embeddings)\n",
    "\n",
    "print(\"‚úì Linear model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf02e65",
   "metadata": {},
   "source": [
    "### 7.5 Evaluate Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec839765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(test_df[\"score_norm\"].values, test_predictions)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä BASELINE MODEL PERFORMANCE (Linear Regression)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_df[\"score_norm\"].values, test_predictions, \n",
    "            marker='.', alpha=0.5, s=10, color='steelblue', edgecolors='none')\n",
    "plt.xlabel(\"True DMS Scores\", fontsize=12)\n",
    "plt.ylabel(\"Predicted DMS Scores\", fontsize=12)\n",
    "plt.title(\"Baseline: Linear Regression on Frozen Embeddings\", fontsize=14, fontweight='bold')\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.plot([-3, 3], [-3, 3], 'r--', linewidth=2, label='Perfect prediction')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Performance is limited because embeddings weren't optimized for this task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a67420",
   "metadata": {},
   "source": [
    "## üöÄ Step 8: Fine-Tuning with LoRA\n",
    "\n",
    "The baseline results aren't great because the frozen embeddings weren't optimized for predicting DMS scores.\n",
    "\n",
    "**Solution**: Fine-tune the model! But training all 8M parameters is expensive...\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** to the rescue! ü¶∏\n",
    "- Freezes pre-trained weights\n",
    "- Injects small trainable \"adapter\" matrices\n",
    "- Reduces trainable parameters by ~99%\n",
    "- Much faster and more memory-efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c981ab-864e-40a3-bbc5-872cc3302b9a",
   "metadata": {},
   "source": [
    "### 8.1 Configure LoRA Adapters\n",
    "\n",
    "**LoRA Parameters:**\n",
    "- **`r`**: Rank of adapter matrices (lower = fewer parameters)\n",
    "- **`lora_alpha`**: Scaling factor for adapter updates\n",
    "- **`target_modules`**: Which layers to adapt (usually attention layers)\n",
    "- **`lora_dropout`**: Dropout rate for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e56ee-a992-4527-8f74-dbd30f5f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Count and display trainable vs total parameters.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    trainable_pct = 100 * trainable_params / all_param\n",
    "    print(f\"  Trainable params: {trainable_params:,}\")\n",
    "    print(f\"  Total params: {all_param:,}\")\n",
    "    print(f\"  Trainable: {trainable_pct:.2f}%\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                              # Rank (lower = fewer params)\n",
    "    lora_alpha=32,                    # Scaling factor\n",
    "    target_modules=[\"query\", \"value\"], # Adapt attention layers\n",
    "    lora_dropout=0.1,                 # Regularization\n",
    "    bias=\"none\",                      # Don't adapt bias terms\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"üîÑ Applying LoRA adapters to model...\\n\")\n",
    "ft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"üìä Model Parameters Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Before LoRA (original model):\")\n",
    "print_trainable_parameters(AutoModel.from_pretrained(MODEL_CHECKPOINT))\n",
    "print(\"\\nAfter LoRA (adapted model):\")\n",
    "print_trainable_parameters(ft_model)\n",
    "print(\"-\" * 60)\n",
    "print(\"\\n‚úì LoRA adapters applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceff2c7",
   "metadata": {},
   "source": [
    "### 8.2 Create Regression Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cfb6c-e6d2-4746-a91e-2d934da1efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Create custom model with regression head\n",
    "class RegressionModel(nn.Module):\n",
    "    \"\"\"Combines LoRA-adapted ESM model with regression head.\"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model  # LoRA-wrapped model\n",
    "        self.regressor = nn.Linear(base_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.base_model(**inputs)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output).squeeze(-1)\n",
    "\n",
    "# Instantiate the complete model\n",
    "regression_model = RegressionModel(ft_model).to(device)\n",
    "\n",
    "print(\"üéØ Complete Model Architecture:\")\n",
    "print(\"-\" * 60)\n",
    "print_trainable_parameters(regression_model)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create data loaders\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "train_dataset = SequenceDataset(train_df, tokenizer)\n",
    "val_dataset = SequenceDataset(val_df, tokenizer)\n",
    "test_dataset = SequenceDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Setup optimizer and loss\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, regression_model.parameters()), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"\\n‚úì Model and data loaders ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c1bc1",
   "metadata": {},
   "source": [
    "### 8.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üöÄ STARTING LORA FINE-TUNING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "for ep in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    regression_model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {ep+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = regression_model(**inputs)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_train_loss = train_loss_sum / len(train_loader)\n",
    "    print(f\"Epoch {ep+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì Fine-tuning complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c212ef6",
   "metadata": {},
   "source": [
    "### 8.4 Evaluate Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üîÑ Evaluating on test set...\\n\")\n",
    "regression_model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        preds = regression_model(**inputs).cpu().numpy()\n",
    "        test_predictions.extend(preds)\n",
    "test_predictions = np.array(test_predictions)\n",
    "\n",
    "# Calculate performance\n",
    "mse_finetuned = mean_squared_error(test_df[\"score_norm\"].values, test_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä FINE-TUNED MODEL PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test Mean Squared Error: {mse_finetuned:.4f}\")\n",
    "print(f\"Baseline MSE: {mse:.4f}\")\n",
    "print(f\"Improvement: {((mse - mse_finetuned) / mse * 100):.1f}% reduction in error\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_df[\"score_norm\"].values, test_predictions, \n",
    "            marker='.', alpha=0.5, s=10, color='seagreen', edgecolors='none')\n",
    "plt.xlabel(\"True DMS Scores\", fontsize=12)\n",
    "plt.ylabel(\"Predicted DMS Scores\", fontsize=12)\n",
    "plt.title(\"Fine-Tuned Model with LoRA\", fontsize=14, fontweight='bold')\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.plot([-3, 3], [-3, 3], 'r--', linewidth=2, label='Perfect prediction')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Much better performance after fine-tuning! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c8606",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaway\n",
    "\n",
    "Even after just **10 epochs** of fine-tuning, the model's performance improved significantly! With more training and hyperparameter tuning, you can achieve even better results.\n",
    "\n",
    "**Why LoRA is powerful:**\n",
    "- ‚úÖ Trains only ~1% of parameters\n",
    "- ‚úÖ Fast and memory-efficient\n",
    "- ‚úÖ Prevents catastrophic forgetting\n",
    "- ‚úÖ Easy to swap different adapters for different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d986b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU or MPS memory\n",
    "print(\"üßπ Cleaning up device memory...\")\n",
    "del model\n",
    "del ft_model\n",
    "del regression_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif getattr(torch, 'mps', None) is not None and torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "print(\"‚úì Memory cleaned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fcb55c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Tutorial Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "1. ‚úÖ Loaded and explored pre-trained ESM-2 models\n",
    "2. ‚úÖ Generated protein embeddings\n",
    "3. ‚úÖ Performed zero-shot similarity search\n",
    "4. ‚úÖ Created a baseline with frozen embeddings\n",
    "5. ‚úÖ Fine-tuned with LoRA for significant improvement\n",
    "\n",
    "**Next steps to explore:**\n",
    "- üîß Experiment with different LoRA configurations (`r`, `lora_alpha`)\n",
    "- üìä Try larger ESM-2 models for better performance\n",
    "- üß™ Apply to different protein property prediction tasks\n",
    "- üî¨ Compare with other fine-tuning strategies\n",
    "- üìà Add validation set monitoring and early stopping\n",
    "\n",
    "**Remember**: This workflow is not limited to biology! The same approach works for:\n",
    "- Natural language processing\n",
    "- Computer vision\n",
    "- Time series analysis\n",
    "- Any domain with large pre-trained models\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
